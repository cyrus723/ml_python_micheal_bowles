# Building Ensemble Models with Python
## solving regression problems with python ensemble packages
* Building a random forest model to predict wine tastes
    * python scikit-learn ensemble module 
* Constructing a RandomForestRegressor object
    * __n_estimators__: Number of trees in the ensemble, generally needs > 10
     trees to gain best performance, appropriate number depends on problem 
     complexity
    * max_depth: depth of trees, we can specify max_leaf_nodes or 
    min_samples_split to control the depth, as it affects training time
        * min_samples_split: nodes having fewer than min_samples_split will 
        not be split anymore
        * min_samples_leaf: if split results in nodes with fewer than 
        min_samples_leaf, then split will not occur. Assigning a value to 
        this attribute requires some thoughts 
        * max_features:: The number of features to consider when looking for 
        best split depends on this value. 
            * default = n_features
            * recommended: sqrt(n_features)
            * if max_features is left to default value, it means that the 
            aglo is performing bagging rather than choosing random set of 
            variables
        * random_state: instance of RandomState, used as random number generator
    *  feature_importances: array of length equal to number of features in 
    the problem
        * each value (positive float) corresponds to the importance of given 
        attribute
        * importances calculated using the method proposed in original 
        variable. The importance is assigned based on the accuracy achieved 
        by the use of given variable
        * fit(x_train, y_train, sample_weight=None): 
            * sample_weight: assign different values for each 
            attribute. default=equal weighting of all input instances
        * predict(x_test): number of columns should be same as training but 
        number of rows can be different
* Modeling wine taste with RandomForestRegressor
    * During model development, random_state can be set specifically to get 
    same kind of results in every run 
    * during real model training, set the random value to default. so that 
    a specified random_state does not affect the final model
* Visualizing the performance of a random forests regression model
    * More trees -> decrease error, also the statistical fluctuations decrease
### Using gradient boosting to predict wine taste
* Takes error_minimization approach for building an ensemble of trees
* shares some tree-related parameters
* Choice of tree-depth and other rationale is different in boosting compared 
to random forests and bagging
* Attributes:
    * loss: Gradient boosting uses trees to approximate gradient of overall 
    loss function. Choices, sum squared error, least sum squared error, sum 
    of absolute values of errors, least mean aboslute value (less sensitive 
    to outliers)
    * learning_state: eps, step size for learning, optimal value is problem 
    dependent, default=0.1
        * larger learning rate -> fewer trees to be trained
    * n_estimators: number of trees in ensemble
        * n_estimators = number of terms in addative approximation (su of 
        trained models)
    * subsample: When gradient boosting is trained on subsample of data, it 
    becomes stochastic gradient boosting 
    * max_depth: GB can get good fidelity with tree_depth=1 because it 
    reduces residual error, tree depth is needed to study the interaction 
    between attributes
    * max_features: same role as random forest
    * warm_start: if true, fit() function subsequent applications start from 
    last stopping point in training and continue to acccumulate the results 
    of adding furtjer gradient steps
    * feature_importances: 
    * train_score: array with length equal to number of trees in the 
    ensemble, containing error on training set at each stage in sequence of 
    trees
    * fit(x_train, y_train, monitor=None): monitor is callable that can be 
    used to stop training early. 
    * predict(X):  generate predictions
    * stage_predict(): iterable predict, each call generates a prediction 
    incorporating one additional tree in the sequence generated by Gradient 
    Boosting
* Getting the parameters:
    * Start with defaults, except set subsample=0.5.  Train model and look at
     curve of out-of-sample performance versus number of trees. 
    * look at shape of oos performance after few iterations, and if oos 
    performance is moving rapidly at the right end, either increase 
    n_estimators or increase learning_rate
    * if oos_performance is going down rapidly, decrease learning rate
    * if oos performance improves and levels out at the right side of the 
    graph, try altering max_depth and max_features
* Accessing the performance of a gradient boosting model
## Coding Bagging to Predict Wine Taste
* Bagging is purely a variance reduction technique
* useful to compare the performance that bagging achieves with performance of
 random forest and gradient boosting
* attributes: 
    * bag_fract: determine how many samples are taken for training
        * recommended: bootstrap samples must be same size as original data i
        .e., bag_fract=1.0
        
## Incorporating Non-numeric attributes in Python Ensemble Models 
* Coding the sex of abalone for input to Random Forest Regression in Python
    * Use n-1 values for the categorical attribute
* Accessing performance and the importance of coded variables    
    
* **mean square variation ~= (standard_deviation)^2**
* Coding sex of abalone for gradient boosting regression in Python
* Accessing performance and the importance of coded variables with Gradient 
Boosting
## Solving binary classification problems with Python Ensemble
* Detecting unexploded mines with python Random Forest
* RandomForestClassifier
    * criterion: possible values for node impurity
    * fit: 
    * predict:
    * predict_proba: produces a two-dimensional array where rows correspond 
    to samples and columns count is equal to number of classes being 
    predicted. Each entry is the probability of the associated class.
    * predict_log_proba: same like predict_proba except the there are logs of
     probability
* Constructing a random forests model to detect unexploded mines
* performance measure
    * ROC curve (AUC)
* Determining the performance of a Random Forests Classifier   
    * For AUC: larger is bette. 1.0 being perfect score
    * Random forests: do not overfit, reduce variance
* Detecting unexploded mines with Python Gradient Boosting
    * Attributes 
        * loss: Deviance is default and only option
        * fit
        * decision_function: generally there is a real number estimate 
        related to the probability of class membership. The real number 
        estimates are passed through an inverse logistic function to turn 
        them into probabilities. 
        * predict
        * predict_proba
        * staged_decision_function
        * staged_predict
        * staged_predict_proba
    * Gradient boosting can overfit
        * better to use ROC curve and the tables of false positives, false 
        negatives
* Determining the performance of gradient boosting classifier
    * Deviance is related to how far the probability estimates are from the 
    correct. but differs slightly from misclassification error
        * gradient boosting tries to improve this quantity
    * GB does get better results when using Random Forest trees.
## Solving Multiclass classification problems with Python Ensemble Methods
* Predicting methods either predict labels of the class label or predict the 
probability of being that class
* Classifying glass with random forests
* Dealing with Class Imbalances
    * Imbalanced class problem: solved by stratified sampling
* Classifying glass with gradient boosting
* Assessing the advantage of using Random Forest base learners with Gradient 
Boosting


## Comparing Algorithms
* Single held out error calculation require 1 training pass where as n-fold 
cross validation requires n training passes.
* penalized linear regression is way much faster
* performance of RandomForest and Gradient Boosting is better
    * RF and GB have nearly same performance, as well as training times
    
